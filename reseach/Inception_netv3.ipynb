{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch can use the GPU.\n",
      "Number of available GPUs: 1\n",
      "Current GPU device: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use the GPU.\")\n",
    "    print(\"Number of available GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU device:\", torch.cuda.current_device())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Before starting the training loop\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:/Users/91623/Desktop/Brain_tumor_research/Data\"\n",
    "train_path = \"C:/Users/91623/Desktop/Brain_tumor_research/Data/Training\"\n",
    "test_path = \"C:/Users/91623/Desktop/Brain_tumor_research/Data/Testing\"\n",
    "image_size = (256,256)\n",
    "num_classes = 5\n",
    "train_paths = glob(f\"{train_path}/*/*.jpg\")\n",
    "test_paths = glob(f\"{test_path}/*/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91623\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\91623\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/179, Loss: 1.4022, Accuracy: 12.50%\n",
      "Batch 150/179, Loss: 1.0434, Accuracy: 72.06%\n",
      "Epoch 1/30, Train Loss: 1.0275, Train Accuracy: 73.53%\n",
      "Epoch 1/30, Validation Loss: 0.9437, Validation Accuracy: 80.17%\n",
      "Batch 0/179, Loss: 0.9459, Accuracy: 81.25%\n",
      "Batch 150/179, Loss: 0.9191, Accuracy: 82.97%\n",
      "Epoch 2/30, Train Loss: 0.9162, Train Accuracy: 83.16%\n",
      "Epoch 2/30, Validation Loss: 0.9179, Validation Accuracy: 82.15%\n",
      "Batch 0/179, Loss: 0.9034, Accuracy: 84.38%\n",
      "Batch 150/179, Loss: 0.9143, Accuracy: 82.72%\n",
      "Epoch 3/30, Train Loss: 0.9132, Train Accuracy: 82.90%\n",
      "Epoch 3/30, Validation Loss: 0.9287, Validation Accuracy: 81.08%\n",
      "Batch 0/179, Loss: 0.9114, Accuracy: 84.38%\n",
      "Batch 150/179, Loss: 0.8968, Accuracy: 84.64%\n",
      "Epoch 4/30, Train Loss: 0.9001, Train Accuracy: 84.28%\n",
      "Epoch 4/30, Validation Loss: 0.9054, Validation Accuracy: 83.30%\n",
      "Batch 0/179, Loss: 0.8786, Accuracy: 87.50%\n",
      "Batch 150/179, Loss: 0.8940, Accuracy: 84.79%\n",
      "Epoch 5/30, Train Loss: 0.8969, Train Accuracy: 84.58%\n",
      "Epoch 5/30, Validation Loss: 0.9067, Validation Accuracy: 82.84%\n",
      "Batch 0/179, Loss: 0.9015, Accuracy: 81.25%\n",
      "Batch 150/179, Loss: 0.8992, Accuracy: 84.23%\n",
      "Epoch 6/30, Train Loss: 0.9020, Train Accuracy: 83.93%\n",
      "Epoch 6/30, Validation Loss: 0.9258, Validation Accuracy: 81.62%\n",
      "Batch 0/179, Loss: 0.9002, Accuracy: 81.25%\n",
      "Batch 150/179, Loss: 0.8859, Accuracy: 85.58%\n",
      "Epoch 7/30, Train Loss: 0.8873, Train Accuracy: 85.49%\n",
      "Epoch 7/30, Validation Loss: 0.9240, Validation Accuracy: 81.16%\n",
      "Batch 0/179, Loss: 0.8767, Accuracy: 84.38%\n",
      "Batch 150/179, Loss: 0.8953, Accuracy: 84.48%\n",
      "Epoch 8/30, Train Loss: 0.8944, Train Accuracy: 84.58%\n",
      "Epoch 8/30, Validation Loss: 0.8886, Validation Accuracy: 85.43%\n",
      "Batch 0/179, Loss: 0.8643, Accuracy: 87.50%\n",
      "Batch 150/179, Loss: 0.8968, Accuracy: 84.40%\n",
      "Epoch 9/30, Train Loss: 0.8986, Train Accuracy: 84.21%\n",
      "Epoch 9/30, Validation Loss: 0.8861, Validation Accuracy: 85.58%\n",
      "Batch 0/179, Loss: 0.8312, Accuracy: 90.62%\n",
      "Batch 150/179, Loss: 0.8919, Accuracy: 85.00%\n",
      "Epoch 10/30, Train Loss: 0.8882, Train Accuracy: 85.36%\n",
      "Epoch 10/30, Validation Loss: 0.8827, Validation Accuracy: 85.96%\n",
      "Batch 0/179, Loss: 0.8274, Accuracy: 93.75%\n",
      "Batch 150/179, Loss: 0.8801, Accuracy: 86.09%\n",
      "Epoch 11/30, Train Loss: 0.8811, Train Accuracy: 86.06%\n",
      "Epoch 11/30, Validation Loss: 0.8978, Validation Accuracy: 83.98%\n",
      "Batch 0/179, Loss: 0.9040, Accuracy: 84.38%\n",
      "Batch 150/179, Loss: 0.8793, Accuracy: 86.15%\n",
      "Epoch 12/30, Train Loss: 0.8792, Train Accuracy: 86.22%\n",
      "Epoch 12/30, Validation Loss: 0.8743, Validation Accuracy: 86.35%\n",
      "Batch 0/179, Loss: 0.7905, Accuracy: 93.75%\n",
      "Batch 150/179, Loss: 0.8887, Accuracy: 85.18%\n",
      "Epoch 13/30, Train Loss: 0.8864, Train Accuracy: 85.50%\n",
      "Epoch 13/30, Validation Loss: 0.8912, Validation Accuracy: 84.74%\n",
      "Batch 0/179, Loss: 0.8377, Accuracy: 90.62%\n",
      "Batch 150/179, Loss: 0.8863, Accuracy: 85.49%\n",
      "Epoch 14/30, Train Loss: 0.8878, Train Accuracy: 85.38%\n",
      "Epoch 14/30, Validation Loss: 0.8821, Validation Accuracy: 86.12%\n",
      "Batch 0/179, Loss: 0.9476, Accuracy: 78.12%\n",
      "Batch 150/179, Loss: 0.8819, Accuracy: 85.84%\n",
      "Epoch 15/30, Train Loss: 0.8794, Train Accuracy: 86.06%\n",
      "Epoch 15/30, Validation Loss: 0.8896, Validation Accuracy: 85.35%\n",
      "Batch 0/179, Loss: 0.8819, Accuracy: 84.38%\n",
      "Batch 150/179, Loss: 0.8876, Accuracy: 85.35%\n",
      "Epoch 16/30, Train Loss: 0.8870, Train Accuracy: 85.31%\n",
      "Epoch 16/30, Validation Loss: 0.8858, Validation Accuracy: 85.43%\n",
      "Batch 0/179, Loss: 0.9353, Accuracy: 81.25%\n",
      "Batch 150/179, Loss: 0.8829, Accuracy: 85.80%\n",
      "Epoch 17/30, Train Loss: 0.8869, Train Accuracy: 85.45%\n",
      "Epoch 17/30, Validation Loss: 0.9114, Validation Accuracy: 83.30%\n",
      "Batch 0/179, Loss: 0.8569, Accuracy: 87.50%\n",
      "Batch 150/179, Loss: 0.8863, Accuracy: 85.49%\n",
      "Epoch 18/30, Train Loss: 0.8878, Train Accuracy: 85.47%\n",
      "Epoch 18/30, Validation Loss: 0.9000, Validation Accuracy: 83.45%\n",
      "Batch 0/179, Loss: 0.8701, Accuracy: 87.50%\n",
      "Batch 150/179, Loss: 0.8848, Accuracy: 85.49%\n",
      "Epoch 19/30, Train Loss: 0.8844, Train Accuracy: 85.59%\n",
      "Epoch 19/30, Validation Loss: 0.8954, Validation Accuracy: 84.44%\n",
      "Batch 0/179, Loss: 0.8414, Accuracy: 90.62%\n",
      "Batch 150/179, Loss: 0.8830, Accuracy: 85.95%\n",
      "Epoch 20/30, Train Loss: 0.8812, Train Accuracy: 86.06%\n",
      "Epoch 20/30, Validation Loss: 0.8921, Validation Accuracy: 84.59%\n",
      "Batch 0/179, Loss: 0.8951, Accuracy: 84.38%\n",
      "Batch 150/179, Loss: 0.8805, Accuracy: 86.07%\n",
      "Epoch 21/30, Train Loss: 0.8790, Train Accuracy: 86.27%\n",
      "Epoch 21/30, Validation Loss: 0.8755, Validation Accuracy: 86.73%\n",
      "Batch 0/179, Loss: 0.8414, Accuracy: 90.62%\n",
      "Batch 150/179, Loss: 0.8835, Accuracy: 85.68%\n",
      "Epoch 22/30, Train Loss: 0.8839, Train Accuracy: 85.70%\n",
      "Epoch 22/30, Validation Loss: 0.8844, Validation Accuracy: 85.58%\n",
      "Batch 0/179, Loss: 0.8075, Accuracy: 93.75%\n",
      "Batch 150/179, Loss: 0.8714, Accuracy: 86.78%\n",
      "Epoch 23/30, Train Loss: 0.8761, Train Accuracy: 86.41%\n",
      "Epoch 23/30, Validation Loss: 0.8595, Validation Accuracy: 88.33%\n",
      "Batch 0/179, Loss: 0.8102, Accuracy: 93.75%\n",
      "Batch 150/179, Loss: 0.8805, Accuracy: 85.97%\n",
      "Epoch 24/30, Train Loss: 0.8800, Train Accuracy: 86.06%\n",
      "Epoch 24/30, Validation Loss: 0.8790, Validation Accuracy: 86.19%\n",
      "Batch 0/179, Loss: 0.8976, Accuracy: 84.38%\n",
      "Batch 150/179, Loss: 0.8790, Accuracy: 86.24%\n",
      "Epoch 25/30, Train Loss: 0.8803, Train Accuracy: 86.12%\n",
      "Epoch 25/30, Validation Loss: 0.8865, Validation Accuracy: 85.81%\n",
      "Batch 0/179, Loss: 0.8106, Accuracy: 93.75%\n",
      "Batch 150/179, Loss: 0.8746, Accuracy: 86.67%\n",
      "Epoch 26/30, Train Loss: 0.8770, Train Accuracy: 86.47%\n",
      "Epoch 26/30, Validation Loss: 0.8681, Validation Accuracy: 87.19%\n",
      "Batch 0/179, Loss: 0.8792, Accuracy: 87.50%\n",
      "Batch 150/179, Loss: 0.8775, Accuracy: 86.44%\n",
      "Epoch 27/30, Train Loss: 0.8786, Train Accuracy: 86.38%\n",
      "Epoch 27/30, Validation Loss: 0.8651, Validation Accuracy: 87.80%\n",
      "Batch 0/179, Loss: 0.9121, Accuracy: 81.25%\n",
      "Batch 150/179, Loss: 0.8772, Accuracy: 86.55%\n",
      "Epoch 28/30, Train Loss: 0.8803, Train Accuracy: 86.31%\n",
      "Epoch 28/30, Validation Loss: 0.8863, Validation Accuracy: 85.43%\n",
      "Batch 0/179, Loss: 0.9079, Accuracy: 84.38%\n",
      "Batch 150/179, Loss: 0.8681, Accuracy: 87.31%\n",
      "Epoch 29/30, Train Loss: 0.8642, Train Accuracy: 87.66%\n",
      "Epoch 29/30, Validation Loss: 0.8810, Validation Accuracy: 86.19%\n",
      "Batch 0/179, Loss: 1.0491, Accuracy: 68.75%\n",
      "Batch 150/179, Loss: 0.8646, Accuracy: 87.87%\n",
      "Epoch 30/30, Train Loss: 0.8656, Train Accuracy: 87.80%\n",
      "Epoch 30/30, Validation Loss: 0.8819, Validation Accuracy: 86.04%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the image size and batch size\n",
    "IMAGE_SIZE = 299\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load the InceptionV3 model with pre-trained weights\n",
    "inception = models.inception_v3(pretrained=True)\n",
    "\n",
    "# Freeze the existing weights\n",
    "for param in inception.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layers to match your classification task\n",
    "class CustomInceptionV3(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(CustomInceptionV3, self).__init__()\n",
    "        self.inception = inception\n",
    "        self.inception.fc = nn.Sequential(\n",
    "            nn.Linear(self.inception.fc.in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through InceptionV3\n",
    "        outputs = self.inception(x)\n",
    "        if self.training and self.inception.aux_logits:\n",
    "            return outputs.logits\n",
    "        else:\n",
    "            return outputs\n",
    "\n",
    "# Instantiate the model\n",
    "model = CustomInceptionV3(num_classes=4).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=3, min_lr=0.001)\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = ImageFolder(root=train_path, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataset = ImageFolder(root=test_path, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Training function\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch_number, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_number % 150 == 0:\n",
    "            c_loss = total_loss / (batch_number + 1)\n",
    "            accuracy = total_correct / total_predictions\n",
    "            print(f\"Batch {batch_number}/{len(train_loader)}, Loss: {c_loss:0.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_accuracy = total_correct / total_predictions\n",
    "    \n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    avg_accuracy = total_correct / total_predictions\n",
    "    \n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch()\n",
    "    val_loss, val_accuracy = validate_epoch()\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy*100:.2f}%')\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
